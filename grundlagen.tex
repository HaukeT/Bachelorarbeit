\chapter{Grundlagen zur Latent Dirichlet Allocation}

Die \gls{lda} ist ein generatives Wahrscheinlichkeitsmodell für Textdokumente. \parencite[vgl.][S. 996]{Blei03latentdirichlet} Dokumente werden als zufällige Mischverteilungen über latente Themen dargestellt, wobei jedes Thema eine Wahrscheinlichkeitsverteilung über Worte ist. 

Vereinfacht gesagt werden alle Dokumente mit einer Wahrscheinlichkeit zu vorher unbekannten Themen zugeordnet. Die Themen werden also durch den Algorithmus gefunden. Ein Thema besteht aus der Menge aller in den Dokumenten vorkommenden Wörtern und ihrer Wahrscheinlichkeit das sie zu diesem Thema gehören.


Die Reihenfolge der Dokumente ist nicht relevant. Auch die Reihenfolge der Wörter in den Dokumenten wird nicht beachtet, sondern nur die Häufigkeit, es gilt das \gls{bow} Modell. \parencite[vgl.][S. 155-156]{harris1954distributional} Die Anzahl der latenten Themen muss vorher gegeben sein.


Um die Anzahl an versteckten Themen zu approximieren werden alle \gls{lda} Modelle mit den Themenanzahlen von 1 bis 100 erstellt. Diese Modelle werden anhand ihrer Kohärenz innerhalb der Themen und anhand ihrer Distanz zwischen den Themen verglichen. Mithilfe dieser Daten sucht man ein Modell aus, das eine möglichst geringe Themenanzahl, hohe Kohärenz und hohe Distanz aufweist. Die Themenanzahl sollte möglichst gering sein, weil es aufwändig ist diese Themen zu interpretieren und die Distanz bei zu hoher Themenzahl sinkt.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=\textwidth,height=8cm,keepaspectratio=true]{lda.png}
	\caption{
		LDA als graphisches Modell, von \parencite[vgl.][S. 23]{ProbabilisticTopicModels}
	}
	\label{fig:LDA Modell}
\end{figure}


$\mathcal{W}$ ist das Wort aus $\mathcal{N}$ Wörtern eines Dokuments i. Dieses Dokument i ist eines aus allen Dokumenten $\mathcal{M}$ bezeichnet als $\mathcal{D}$ \parencite[vgl.][S. 995]{Blei03latentdirichlet}. Alle folgenden Parameter sind latent. $\mathcal{Z}$ ist das Thema für das Wort j aus besagtem Dokument i \parencite[vgl.][S. 996-997]{Blei03latentdirichlet} \parencite[vgl.][S. 27]{ProbabilisticTopicModels}. Jedem Wort wird ein Thema zugeordnet. Wodurch jedes Dokument eine Mischung aus allen Themen ist. Die Verteilung der Themen für Dokument i ist $\theta$. $\alpha$ und $\beta$ sind Hyperparameter des \gls{lda} \parencite[vgl.][S. 32]{ProbabilisticTopicModels}. $\alpha$ bestimmt die Dokument-Themen Verteilung und die Wort-Themen Verteilung. Ein hoher $\alpha$ Wert erhöht die Wahrscheinlichkeit dafür das einem Dokument mehr Themen zugeordnet werden. Ein niedriger $\alpha$ Wert verringert die Wahrscheinlichkeit das einem Dokument mehrere Themen zugeordnet werden. Ein hoher $\beta$ Wert erhöht die Wahrscheinlichkeit das einem Thema mehr Wörter zugeordnet werden. Ein niedriger $\beta$ Wert erhöht die Wahrscheinlichkeit das einem Thema weniger Wörter zugeordnet werden. Vereinfacht gesagt lässt ein großer $\alpha$ Wert die Dokumente ähnlicher aussehen und ein hoher $\beta$ Wert lässt die Themen ähnlicher aussehen. Mit diesem Algorithmus lässt sich ein Model erstellen, das jedes Wort mit Wahrscheinlichkeit zu jedem Thema zuordnet.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=\textwidth,height=3cm,keepaspectratio=true]{ldaDocLikelyhood.png}
	\caption{
		LDA als graphisches Modell, Knoten sind zufällige Variablen, Kanten zeigen Abhängigkeit, der graue Knoten wird beobachtet, die Rechtecke sind Wiederholungen \parencite[vgl.][S. 25]{ProbabilisticTopicModels}
	}
	\label{fig:LDA_Formel}
\end{figure}


Die Abbildung \ref{fig:LDA_Formel} zeigt die Wahrscheinlichkeit ein Dokument zu generieren, mit den Einstellungen des \gls{lda} Modells. Die Wahrscheinlichkeit ist gering aber je höher sie ist, desto besser ist das Modell. Die vier Komponenten der Formel sind die Einstellungen des \gls{lda} Modells als Faktoren. Diese ergeben wiederum eigene Wahrscheinlichkeiten. Der Erste Faktor ist eine Dirichletverteilung von Dokumenten zu Themen. Eine Dirichletverteilung kann man sich als n-Simplex vorstellen, mit n gleich der Anzahl von Themen \parencite[vgl.][S. 53-54]{ProbabilisticTopicModels}. Jedes Dokument hat eine Wahrscheinlichkeit für die Zugehörigkeit zu jedem Thema. Die Dirichletverteilung ist also eine Verteilung von Verteilungen. Der zweite Faktor ist eine Dirichletverteilung von Themen zu Wörtern und verhält sich analog zur ersten \parencite[vgl.][S. 996-999]{Blei03latentdirichlet}.

Der dritte Faktor ist eine Multinomialverteilung des ersten Faktors \parencite[vgl.][S. 999-1002]{Blei03latentdirichlet}. Eine Multinomialverteilung ist wie eine Urne mit mehreren verschiedenen Themen, die mit Wahrscheinlichkeiten gezogen werden können. Diese zweite Multinomialverteilung ist also eine von Themen. Der Vierte Faktor ist eine Multinomialverteilung des zweiten Faktors mit Worten. Diese verhält sich analog zur ersten. Kombiniert man diese Multinomialverteilungen miteinander, indem man immer ein Thema aus der ersten zieht und zu dem Thema passend ein Wort aus der zweiten, generiert man ein neues Dokument. Dies wird wiederholt bis gleich viele Dokumente generiert wurden wie verarbeitet wurden. Die Wahrscheinlichkeit das man mit dieser Methode die gleichen Dokumente erzeugt ist wie gesagt gering.

Die Dirichletverteilungen werden mit den $\alpha$ und $\beta$ Werten beeinflusst. Es werden viele verschiedene Werte getestet und das Modell mit der höchsten Wahrscheinlichkeit die gleichen original Dokumente zu erzeugen gewinnt.

\gls{dlda} ist eine Version des \gls{dlda}, welche die chronologische Reihenfolge der Dokumente berücksichtigt. Dadurch ist es möglich die Veränderung der Themenschwerpunkte über den Zeitraum zu betrachten. \parencite[vgl.][]{dynamicLDA}

\gls{hlda} erweitert \gls{lda} durch eine Hierarchie aus Unterthemen, mit beliebiger Tiefe \parencite[vgl.][]{griffiths2004hierarchical}. Diese Hierarchien lassen sich als Baumdiagramm darstellen. Dadurch erhält man noch mehr Informationen zu einem Thema, um es genauer zu benennen. Auch Cluster lassen sich dadurch erkennen. 